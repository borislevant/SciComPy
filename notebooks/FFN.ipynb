{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEEgE7xucUTF"
      },
      "source": [
        "# Feed Forward Network\n",
        "\n",
        "## [Scientific Computing with Python](http://scicompy.yoavram.com)\n",
        "## Yoav Ram\n",
        "\n",
        "In this session we will understand:\n",
        "- what feed forward networks are and how they work,\n",
        "- how to back-propagate gradients in a multi-layer network\n",
        "- how to efficiently fit multi-layer networks using adaptive gradient descent techniques\n",
        "- how to feed backwards to generate instead of classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RCRAa_YzcUTJ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.optimize\n",
        "from scipy.special import expit\n",
        "import scipy.misc\n",
        "import seaborn as sns\n",
        "sns.set(\n",
        "    style='ticks'\n",
        ")\n",
        "from ipywidgets import interact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45l02GyEcUTM"
      },
      "source": [
        "We start by loading the MNIST digits data we used in the [softmax model session](softmax-model.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hB_g9ZCjcUTN"
      },
      "outputs": [],
      "source": [
        "try: \n",
        "    from tensorflow import keras \n",
        "except ImportError:\n",
        "    import keras\n",
        "    \n",
        "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "H1c_Uff9cUTO"
      },
      "outputs": [],
      "source": [
        "nsamples, width, height = X_train.shape\n",
        "nfeatures = width * height\n",
        "X_train = X_train.reshape(-1, nfeatures)\n",
        "X_test = X_test.reshape(-1, nfeatures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "m6uokwqTcUTP"
      },
      "outputs": [],
      "source": [
        "def onehot(labels):\n",
        "    nsamples = len(labels)\n",
        "    Y = np.zeros((nsamples, len(np.unique(labels))))\n",
        "    Y[np.arange(nsamples), labels] = 1\n",
        "    return Y\n",
        "\n",
        "Y_train = onehot(Y_train)\n",
        "Y_test = onehot(Y_test)\n",
        "ncats = Y_test.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YNU-RaNEcUTQ",
        "outputId": "b3b20067-ac8a-4b87-d95d-3d77d919d23d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADtCAYAAABTTfKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAG3klEQVR4nO3dT4iOawPH8Zs5MZqaJI/kTyQ2Grup2RArNqQUqSFqsrKwkYU/JaEsbNBsLVBYscQKK7EcOxY2mjyUkgyZnndxcjK973Pd7zN/zPnNfD5Lv65n7s3XfY6rmVnQarVaFRBj4Ww/ANAZ0UIY0UIY0UKYvzo9MDY2Vo2MjFSNRqPq6uqaiWeCeW98fLxqNptVX19f1d3dPWHrONqRkZFqcHBw2h4OaO/OnTtVf3//hD/rONpGo/HPh61cuXJ6ngyYYHR0tBocHPynt991HO2v/yReuXJltWbNmqk/HdDW//pfUP8QBWFEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2FEC2H+mu0H4P/z4sWL4n7r1q3i/uzZs+I+MjLS8TP9cvXq1eK+atWq4v78+fPifvjw4bbbwMBA8exc5E0LYUQLYUQLYUQLYUQLYUQLYUQLYdzT/ovcu3ev7XbixIni2WazWdxbrVZx37FjR3H/+PFj2+3kyZPFs3Xqnq30te/evTulr53ImxbCiBbCiBbCiBbCiBbCiBbCiBbCuKedRj9//izuL1++LO7Hjh1ru339+rV4dvv27cX93LlzxX3r1q3F/fv37223AwcOFM8+evSouNfp7++f0vm5xpsWwogWwogWwogWwogWwogWwogWwrinnUa3b98u7kNDQ5P+7J07dxb30vfiVlVV9fb2Tvpr133+VO9h165dW9yPHDkypc+fa7xpIYxoIYxoIYxoIYxoIYxoIYxoIYx72g6cPXu2uF++fLm4L1iwoLgfP3687Xbx4sXi2anew9a5dOnSjH32tWvXinuj0Zixr53ImxbCiBbCiBbCiBbCiBbCiBbCuPL5zYULF4p73ZXO4sWLi/uuXbuK+5UrV9puS5YsKZ6tMzY2VtwfP35c3N+9e9d2q/tVlXU/vnXv3r3FnYm8aSGMaCGMaCGMaCGMaCGMaCGMaCHMvLun/fz5c9tteHi4eLbuW+vq7mEfPHhQ3KfizZs3xX1wcLC4v3r1atJfe//+/cX91KlTk/5s/ps3LYQRLYQRLYQRLYQRLYQRLYQRLYSZd/e0P378aLs1m80pfXbdjwL98OFDcb9582bb7eHDh8Wzr1+/Lu5fvnwp7nV30AsXtv/7/dChQ8WzPT09xZ3OeNNCGNFCGNFCGNFCGNFCGNFCGNFCmHl3T7to0aK224oVK4pn6+5Z169fX9zr7kKnYvXq1cW97ldhvn//vrgvX7687bZnz57iWaaXNy2EES2EES2EES2EES2EES2EES2EmXf3tEuXLm271f1c4t27dxf3T58+FfeNGzcW99LvaT169Gjx7LJly4r7wYMHi3vdPW3def4cb1oII1oII1oII1oII1oII1oIM++ufEoGBgaK+1R/xOpMevbsWXF/+vRpca/7tsENGzZ0/EzMDG9aCCNaCCNaCCNaCCNaCCNaCCNaCOOedo749u1bca+7h63bfWvev4c3LYQRLYQRLYQRLYQRLYQRLYQRLYRxTztH7Nq1a7YfgT/EmxbCiBbCiBbCiBbCiBbCiBbCiBbCuKedIx49ejTbj8Af4k0LYUQLYUQLYUQLYUQLYUQLYVz5zBFv376d7UfgD/GmhTCihTCihTCihTCihTCihTCihTDuaeeIbdu2FfdWq/WHnoSZ5k0LYUQLYUQLYUQLYUQLYUQLYUQLYdzTzhFbtmwp7ps2bSrudd+PW9objUbxLNPLmxbCiBbCiBbCiBbCiBbCiBbCiBbCuKedJ06fPl3ch4aGJn3+xo0bxbObN28u7nTGmxbCiBbCiBbCiBbCiBbCiBbCiBbCuKedJ/bt21fc7969W9yfPHnSdjt//nzx7M2bN4t7T09PcWcib1oII1oII1oII1oII1oII1oI48pnnujt7S3u9+/fL+5nzpxpuw0PDxfP1l0J+da9znjTQhjRQhjRQhjRQhjRQhjRQhjRQhj3tFRVVX+Pe/369UltTD9vWggjWggjWggjWggjWggjWgjT8ZXP+Ph4VVVVNTo6Ou0PA/ztV1+/evtdx9E2m82qqqpqcHBwio8F1Gk2m9W6desm/NmCVqvV6uRDxsbGqpGRkarRaFRdXV3T+oDA38bHx6tms1n19fVV3d3dE7aOowVml3+IgjCihTCihTCihTCihTCihTCihTCihTCihTCihTD/AfTyFLUwiIZ6AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def display_image(im):\n",
        "    plt.imshow(im.reshape((width, height)), cmap='gray_r')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "display_image(X_train[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8JragCvcUTU"
      },
      "source": [
        "With softmax regression we got accuracy of ~87% (or even up to 92% if we run it longer, see the scikit-learn example).\n",
        "But we talked about the problems with logistic models and that it might be that the relationship between pixels and categories is not linear or even monotone.\n",
        "We also discussed a way to overcome this issue - polynomial features.\n",
        "\n",
        "Here we will use another solution - the layering or composition of several linear models.\n",
        "\n",
        "The idea is that for an input (features) $\\mathbf{X_1}$, the model we will use is:\n",
        "\n",
        "$$\n",
        "\\mathbf{X_2} = f_1(\\mathbf{X_1} \\cdot  \\mathbf{W_1}) \\\\\n",
        "\\mathbf{X_3} = f_2(\\mathbf{X_2} \\cdot  \\mathbf{W_2}) \\\\\n",
        "\\ldots \\\\\n",
        "\\mathbf{\\widehat Y} = f_{n}(\\mathbf{X_n})\n",
        "$$\n",
        "\n",
        "We will fit this model to the data by looking for $\\mathbf{W_1}, \\ldots, \\mathbf{W_n}$ that give us a good prediction.\n",
        "\n",
        "This kind of model is called a **feed forward network**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16DJoBOdcUTV"
      },
      "source": [
        "# Two layers FFN\n",
        "\n",
        "We start with two layers - the readout layer is the softmax regression layer, and before that we will have a hidden layer that transforms the input features $\\mathbf{X_1}$ into complex features $\\mathbf{X_2}$: each feature in $\\mathbf{X_2}$ is a combination of all features in $\\mathbf{X_1}$.\n",
        "\n",
        "We already implemented these functions in the previous session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "iNWNBWdacUTX"
      },
      "outputs": [],
      "source": [
        "def random_matrix(ninputs, noutputs):\n",
        "    boundary = np.sqrt(6 / (ninputs + noutputs))\n",
        "    return np.random.uniform(-boundary, boundary, size=(ninputs, noutputs))\n",
        "\n",
        "def softmax(x):\n",
        "    expx = np.exp(x - x.max(axis=1, keepdims=True))\n",
        "    return expx / expx.sum(axis=1, keepdims=True)\n",
        "\n",
        "def accuracy(Yhat, Y):\n",
        "    return (Y.argmax(axis=1) == Yhat.argmax(axis=1)).mean()\n",
        "\n",
        "def cross_entropy(Yhat, Y):\n",
        "    ylogy = Y * np.log(Yhat)\n",
        "    return -ylogy.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecQKxqprcUTY"
      },
      "source": [
        "# Exercise: ReLU\n",
        "The readout layer uses the softmax function $f_2(z)=\\mathit{softmax}(z)$.\n",
        "What about $f_1(z)$?\n",
        "\n",
        "To add some non-linearity to the model, we want to use a non-linear function, usually called [*activation function*](https://en.wikipedia.org/wiki/Activation_function).\n",
        "For ease of use, we want a differentiable function that's easy and efficient to compute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XASX-kmzcUTZ"
      },
      "source": [
        "There are several options we can use, and a common choise is the rectifier, or [*ReLU*](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29) function:\n",
        "\n",
        "$$\n",
        "ReLU(x) = \\cases{x, & x>0 \\\\ 0, &  x < 0 \\\\ }\n",
        "$$\n",
        "\n",
        "with a derivative\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx}ReLU(x) = \\cases{1, &  x>0 \\\\ 0, & x < 0 \\\\ }\n",
        "$$\n",
        "\n",
        "Implement these functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "S71rPP4hcUTa"
      },
      "outputs": [],
      "source": [
        "def ReLU(X):\n",
        "    # your code here\n",
        "    ret = X.copy()\n",
        "    ret[ret < 0] = 0\n",
        "    return ret\n",
        "\n",
        "def dReLU(X):\n",
        "    # your code here\n",
        "    ret = X.copy()\n",
        "    ret[ret < 0] = 0\n",
        "    ret[ret > 0] = 1\n",
        "    return ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "KUbMZ59IcUTb",
        "outputId": "adecb65f-920e-4d74-ac96-faa94aee344d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.25915233,  7.60782821, -1.07734774,  3.09899627, -2.27264744])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "ReLU(np.random.uniform(-10, 10, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ASOEQRXOcUTb",
        "outputId": "b5debbc2-94e1-4ea4-faf1-d29533ef7690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.93345985 4.22608938 4.75241386 5.42054287 4.9918954 ]\n",
            "[0.93345985 4.22608938 4.75241386 5.42054287 4.9918954 ]\n",
            "[1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "arr = np.random.uniform(-10, 10, 5)\n",
        "print(arr)\n",
        "print(ReLU(arr))\n",
        "print(dReLU(arr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddyZFeT2cUTc"
      },
      "source": [
        "# Dropout\n",
        "\n",
        "A simple way to avoid overfitting is to use [*dropout*](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf): randomly ignoring some elements of every layer during training (but not during prediction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "SST7u7_hcUTc"
      },
      "outputs": [],
      "source": [
        "def drop(X, keep_prob=1):\n",
        "    if keep_prob < 1:\n",
        "        X = X.copy() # we don't want to change X\n",
        "        keeps = np.random.rand(X.shape[1]) < keep_prob\n",
        "        # X.shape is (nsamples, nfeatures)\n",
        "        X[:, ~keeps] = 0 # ignore\n",
        "        X[:, keeps] *= (1/keep_prob) # normalize\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt-TXt_1cUTd"
      },
      "source": [
        "# Feed Forward\n",
        "\n",
        "Next, we want to write a function that for given input $\\mathbf{X_1}$ calculates $\\mathbf{\\widehat Y}$.\n",
        "But we also want to get all the intermediate layers, not just the final layer (the output, result of the readout layer).\n",
        "The reason will become clear soon, when we write the back-propagation.\n",
        "\n",
        "Prediction is then done by taking the last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Yp02ya0ZcUTd"
      },
      "outputs": [],
      "source": [
        "def feed_forward(Ws, X, keep_prob=1):\n",
        "    X1 = X\n",
        "    W1, W2 = Ws\n",
        "    # hidden layer\n",
        "    Z1 = X1 @ W1 \n",
        "    Z1 = drop(Z1, keep_prob=keep_prob)\n",
        "    X2 = ReLU(Z1) \n",
        "    # readout layer\n",
        "    Z2 = X2 @ W2 \n",
        "    Yhat = softmax(Z2) \n",
        "    return [X1, Z1, X2, Z2, Yhat]\n",
        "\n",
        "def predict(Ws, X):\n",
        "    X = np.atleast_2d(X)\n",
        "    return feed_forward(Ws, X, keep_prob=1)[-1]\n",
        "\n",
        "def display_prediction(idx):\n",
        "    prediction = predict(Ws, X_test[idx, :]).argmax()\n",
        "    print(prediction)\n",
        "    return display_image(X_test[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "5eJlQk2PcUTe",
        "outputId": "e005dde5-7af8-4997-d7af-c3f2f0ff57a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "nhidden = 100\n",
        "W1 = random_matrix(nfeatures, nhidden)\n",
        "W2 = random_matrix(nhidden, ncats)\n",
        "Ws = [W1, W2]\n",
        "\n",
        "display_prediction(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg9Y-EgscUTf"
      },
      "source": [
        "# Back propagation\n",
        "\n",
        "We now arrive at the most complicated and important part: the calculation of the gradeints of the loss function with respect to the $\\mathbf{W_k}$ matrices - the model parameters, the weights of the network.\n",
        "\n",
        "Since we use the softmax function, the loss function is still the cross-entropy function $\\mathbf{J}(\\mathbf{W_1}, \\ldots, \\mathbf{W_n} \\mid \\mathbf{X_1}, \\mathbf{Y})$, the negative of the log-likelihood.\n",
        "\n",
        "We know how to do gradient descent, but we need to calculate the gradient.\n",
        "This seems at first very hard for a model as complex as:\n",
        "$$\n",
        "\\mathbf{Z_1} = \\mathbf{X_1} \\cdot  \\mathbf{W_1} \\\\\n",
        "\\mathbf{X_2} = f_1(\\mathbf{Z_1}) \\\\\n",
        "\\mathbf{Z_2} = \\mathbf{X_2} \\cdot \\mathbf{W_2} \\\\\n",
        "\\mathbf{X_3} = f_2(\\mathbf{Z_2}) \\\\\n",
        "\\ldots \\\\\n",
        "\\mathbf{\\widehat Y} = f_{n}(\\mathbf{Z_n}) \\\\\n",
        "\\mathbf{J} = - \\sum_{j=1}^{nsamples}{ \\sum_{k=1}^{ncats}{y_{j,k} \\log{\\widehat y_{j,k}}} }\n",
        "$$\n",
        "\n",
        "But we actually know how to calculate the gradient at each layer $\\frac{\\partial \\mathbf{X_k}}{\\partial \\mathbf{X_{k-1}}}$ so calculating the gradient with respect to any $\\mathbf{W_k}$ is just a matter of applying the chain rule as many times as we need.\n",
        "\n",
        "The idea is that we first calculate the loss function - **feed forward**.\n",
        "Then we apply the chain rule in reverse iteration to calculate the gradient of the loss function $\\mathbf{J}$ with respect to all $\\mathbf{W_k}$.\n",
        "Since the gradient at any layer is calculated as a function of the error measured in the previous layer, we essentially propagate the error backwards, thus the name for this algorithm - [**back propagation**](https://en.wikipedia.org/wiki/Backpropagation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQuIn5krcUTf"
      },
      "source": [
        "To calculate $\\partial \\mathbf{J}/\\partial W_2$ we have the exact same calculation as in the [logistic](logistic-model.ipynb) or softmax model (makes sense as $\\mathbf{W_2}$ is the softmax layer)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathbf{J}}{\\partial W_2} = \n",
        "\\frac{\\partial \\mathbf{J}}{\\partial \\widehat Y} \\cdot \\frac{\\partial \\widehat Y}{\\partial Z_2} \\cdot  \\frac{\\partial Z_2}{\\partial W_2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdQ995k0cUTf"
      },
      "source": [
        "To calculate $\\partial \\mathbf{J}/\\partial \\mathbf{W_1}$ (which is a matrix with the same number of entries as $\\mathbf{W_1}$) we can use a similar but longer calculation as the readout layer:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W_1}} = \n",
        "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\widehat Y}} \\cdot \\frac{\\partial \\mathbf{\\widehat Y}}{\\partial \\mathbf{Z_2}} \\cdot \\frac{\\partial \\mathbf{Z_2}}{\\partial \\mathbf{X_2}} \\cdot \\frac{\\partial \\mathbf{X_2}}{\\partial \\mathbf{Z_1}} \\cdot \\frac{\\partial \\mathbf{Z_1}}{\\partial \\mathbf{W_1}}\n",
        "$$\n",
        "\n",
        "Note that the first terms of this product were already calculated when calculating $\\frac{\\partial C}{\\partial \\mathbf{W_2}}$; the term \n",
        "\n",
        "$$\n",
        "\\delta_2 = \\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{\\widehat Y}} \\cdot \\frac{\\partial \\mathbf{\\widehat Y}}{\\partial \\mathbf{Z_2}}\n",
        "$$\n",
        "\n",
        "is considered the error of the previous layer, and it is back-propagated to the next layer:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathbf{J}}{\\partial \\mathbf{W_1}} = \n",
        "\\delta_2 \\cdot \\frac{\\partial \\mathbf{Z_2}}{\\partial \\mathbf{X_2}} \\cdot \\frac{\\partial \\mathbf{X_2}}{\\partial \\mathbf{Z_1}} \\cdot \\frac{\\partial \\mathbf{Z_1}}{\\partial \\mathbf{W_1}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdJ0DPv9cUTg"
      },
      "source": [
        "These are three easy opeations:\n",
        "$$\n",
        "\\frac{\\partial \\mathbf{X_2}}{\\partial \\mathbf{Z_1}} = \\frac{\\partial ReLU{(\\mathbf{Z_1})}}{\\partial \\mathbf{Z_1}} = ReLU'(\\mathbf{Z_1}) \\\\\n",
        "\\frac{\\partial \\mathbf{Z_1}}{\\partial \\mathbf{W_1}}=\\frac{\\partial (\\mathbf{X_1} \\mathbf{W_1})}{\\partial \\mathbf{W_1}} = \\mathbf{X_1} \\\\\n",
        "\\frac{\\partial \\mathbf{Z_2}}{\\partial \\mathbf{X_2}} = \\frac{\\partial (\\mathbf{X_2} \\mathbf{W_2})}{\\partial \\mathbf{X_2}} = \\mathbf{W_2}\n",
        "$$\n",
        "\n",
        "In the next layer $\\delta_1$ is updated to \n",
        "$$\n",
        "\\delta_1 = \\delta_2 \\cdot \\frac{\\partial \\mathbf{Z_2}}{\\partial \\mathbf{X_2}} \\cdot \\frac{\\partial \\mathbf{X_2}}{\\partial \\mathbf{Z_1}} = \\delta_2 \\cdot W_2 \\cdot Relu'(Z_1),\n",
        "$$\n",
        "thus *propagating back* the error $\\delta$.\n",
        "\n",
        "Consider that in forwarding we use the matrix multiplication to activate each node as a weighted average of the previous nodes (the weights given by $\\mathbf{W_k}$). \n",
        "Back propagation can be considered the reverse operation.\n",
        "\n",
        "The actual operators used below are chosen to avoid loops (except the loop through the layers which is unavoiable) and to keep the correct dimensions - see the discussion in the [softmax model session](softmax-model.ipynb#Gradient-descent)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "hrId859fcUTh"
      },
      "outputs": [],
      "source": [
        "def back_propagation(Ws, X, Y, keep_prob=1):\n",
        "    W1, W2 = Ws\n",
        "    X1, Z1, X2, Z2, Yhat = feed_forward(Ws, X, keep_prob=keep_prob)\n",
        "\n",
        "    # readout layer\n",
        "    δ2 = Yhat - Y # prediction error, dJ/dYhat * dYhat/dZ2 (-1, ncats)\n",
        "    dW2 = X2.T @ δ2 # dJ/dW2 = δ2 * dZ2/dX2 = δ2 * X2\n",
        "\n",
        "    # hidden layer\n",
        "    δ1 = (δ2 @ W2.T) * dReLU(Z1) # δ1 = dJ/dX_2 = δ2 * dZ2/dX2 * dX2/dZ1 = δ2 * W2 * ReLU(Z1)\n",
        "    dW1 = X1.T @ δ1 # dJ/dW1 = δ1 * dZ1/dW1 = δ1 * X1\n",
        "\n",
        "    gradients = [dW1, dW2]\n",
        "    # sanity checks\n",
        "    assert len(gradients) == len(Ws), (len(gradients), len(Ws))\n",
        "    for dW, W in zip(gradients, Ws):\n",
        "        assert dW.shape == W.shape, (dW.shape, W.shape)\n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "g0jPV9aFcUTh",
        "outputId": "a9a5c688-2ffb-4338-e5df-9be0c5c0de67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "nhidden = 100\n",
        "W1 = random_matrix(nfeatures, nhidden)\n",
        "W2 = random_matrix(nhidden, ncats)\n",
        "Ws = [W1, W2]\n",
        "\n",
        "X, Y = X_train[:5, :], Y_train[:5, :]\n",
        "\n",
        "dW1, dW2 = back_propagation(Ws, X, Y)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
        "axes[0].imshow(dW1.T, cmap='viridis')\n",
        "axes[1].imshow(dW2.T, cmap='viridis')\n",
        "axes[0].set(xticks=[], yticks=[])\n",
        "axes[1].set(xticks=[], yticks=[]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8CFx9XkcUTi"
      },
      "source": [
        "# Gradient checking\n",
        "\n",
        "Writing and implementing the gradient computation can be really hard.\n",
        "But there is a good way to sanity-check that we made the correct computation.\n",
        "\n",
        "Remember that the definition of a derivative is\n",
        "$$\n",
        "\\frac{d f}{d x}\\big(x\\big) = \\lim_{\\Delta \\to 0}{\\frac{f(x+\\Delta) - f(x-\\Delta)}{2\\Delta}}\n",
        "$$\n",
        "Therefore, we can compute the cross entropy at the params, then slightly change each parameter and compute the cross entropy again, and use these two computations as $f(x+\\Delta)$ and $f(x-\\Delta)$ where $x$ is the parameter we changed and $\\Delta$ is the slight change. \n",
        "Then we can compare $\\frac{f(x+\\Delta)-f(x-\\Delta)}{2\\Delta}$ to the gradient computed by back propagation and check that the difference is very small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "LuWOBf0IcUTj",
        "outputId": "c418e684-da5a-469f-f689-0f954f28e85e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W2\n",
            "6 5 5.1684397136534214e-06 5.093170329928398e-06 0.007335046196975781\n"
          ]
        }
      ],
      "source": [
        "def loss(Ws, X, Y):\n",
        "    Yhat = predict(Ws, X)\n",
        "    return cross_entropy(Yhat, Y)\n",
        "\n",
        "def gradient_check(Ws, X, Y, Δ=1e-5):\n",
        "    dWs = back_propagation(Ws, X, Y, keep_prob=1)\n",
        "    Ws_ = [W.copy() for W in Ws]\n",
        "\n",
        "    for i, (W_, dW_) in enumerate(zip(Ws_, dWs)):\n",
        "        print('W{}'.format(i+1))\n",
        "        for i in range(W_.shape[0]):\n",
        "            for j in range(W_.shape[1]):\n",
        "                dw = dW_[i, j]\n",
        "                W_[i,j] += Δ\n",
        "                loss1 = loss(Ws_, X, Y)\n",
        "                W_[i,j] -= 2*Δ\n",
        "                loss2 = loss(Ws_, X, Y)\n",
        "                W_[i,j] += Δ\n",
        "                dw_ = (loss1 - loss2) / (2 * Δ)\n",
        "                rel_error = abs(dw - dw_) / abs(dw + dw_)\n",
        "                if not np.isclose(dw_, dw):\n",
        "                    print(i, j, dw, dw_, rel_error)\n",
        "\n",
        "nhidden = 10\n",
        "W1 = random_matrix(nfeatures, nhidden)\n",
        "W2 = random_matrix(nhidden, ncats)\n",
        "Ws = [W1, W2]\n",
        "X, Y = X_train[:100,:], Y_train[:100,:]\n",
        "gradient_check(Ws, X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17AzPEencUTk"
      },
      "source": [
        "# Momentum gradient descent\n",
        "\n",
        "There are many different ways to do stochastic gradient descent.\n",
        "Some of the algorithms adapt the learning rate $\\eta$ during training. \n",
        "The basic intuition is that when we are far from the minimum we want to advance in big steps and when we are near to the minimum we want to advance more carefully.\n",
        "\n",
        "The first optimization method we will use was described in [Rumelhart, Hinton & Williams, 1986](http://www.nature.com/doifinder/10.1038/323533a0).\n",
        "It is a momentum method, meaning that at each iteration (batch) the current update is calculated as a decaying average of the previous update and the current gradients:\n",
        "\n",
        "$$\n",
        "\\Delta W \\to \\alpha \\Delta W - \\eta \\frac{\\partial J}{\\partial W} \\\\\n",
        "W \\to W + \\Delta W\n",
        "$$\n",
        "\n",
        "This works fine, sometimes, but not as good as other algorithms. It simple and easy to implement, though, and gives a good sense of what other momentum-based algorithms do.\n",
        "See a [comparison of algorithms on MNIST](https://github.com/mazefeng/sgd-opt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "z1E9vyD_cUTl"
      },
      "outputs": [],
      "source": [
        "class MomentumOptimizer:\n",
        "    def __init__(self, α=0.9, η=0.05):\n",
        "        self.α = α\n",
        "        self.η = η\n",
        "        self.ΔWs = None\n",
        "\n",
        "    def send(self, dWs):\n",
        "        if self.ΔWs is None:\n",
        "            self.ΔWs = [0] * len(dWs)\n",
        "        \n",
        "        self.ΔWs = [\n",
        "            self.α * ΔW - self.η * dW\n",
        "            for ΔW, dW in zip(self.ΔWs, dWs)\n",
        "        ]\n",
        "        \n",
        "        for ΔW in self.ΔWs:\n",
        "            assert np.isfinite(ΔW).all()\n",
        "        return self.ΔWs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "UtPIS46ocUTl",
        "outputId": "80558e0a-6a75-49ec-f0c3-b843f0aacde8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0]\n",
            "[-0.05, -0.05, -0.05]\n",
            "[-0.095, -0.095, -0.095]\n"
          ]
        }
      ],
      "source": [
        "optimizer = MomentumOptimizer()\n",
        "print(optimizer.send([0, 0, 0]))\n",
        "print(optimizer.send([1, 1, 1]))\n",
        "print(optimizer.send([1, 1, 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GFA7IEicUTl"
      },
      "source": [
        "# Adam optimizer\n",
        "\n",
        "A more sophisticated optimizing algorithm is [Adam](https://arxiv.org/pdf/1412.6980v9.pdf).\n",
        "The algorithm to calculate the learning rate is in the linked paper in *Algorithm 1*, and the code below uses a slightly different order of compuation for efficieny.\n",
        "\n",
        "Intuitively, we calculate the weighted running averages of the gradients $\\mathbf{m}$ and of the squared gradeints $\\mathbf{v}$, with memory parameters $\\beta_1, \\beta_2$ such that the most recent gradient has a weight of $1-\\beta_1$ and $1-\\beta_2$:\n",
        "\n",
        "$$\n",
        "\\mathbf{m} \\to \\beta_1 \\mathbf{m} + (1-\\beta_1) \\frac{\\partial C}{\\partial \\mathbf{W}} \\\\\n",
        "\\mathbf{v} \\to \\beta_2 \\mathbf{v} + (1-\\beta_2) \\Big(\\frac{\\partial C}{\\partial \\mathbf{W}}\\Big)^2\n",
        "$$\n",
        "\n",
        "The ratio of $\\frac{\\mathbf{m}}{\\sqrt{\\mathbf{v}}}$ is similar to a *signal-to-noise* ratio, and we use this ratio instead of the gradients in a gradient descent scheme to update the parameters $\\mathbf{W}$:\n",
        "\n",
        "$$\n",
        "\\mathbf{W} \\to  \\mathbf{W} - \\alpha \\frac{\\mathbf{m}}{\\sqrt{\\mathbf{v}}}\n",
        "$$\n",
        "\n",
        "See the [paper by Kingma and Ba](https://arxiv.org/pdf/1412.6980v9.pdf) for more details and comparison to other optimization strategies.\n",
        "We use **Algorithm 1** from [Kingma and Ba](https://arxiv.org/pdf/1412.6980v9.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "eWgNGknucUTm"
      },
      "outputs": [],
      "source": [
        "def average(prev, curr, β): \n",
        "    return [\n",
        "        β * p + (1 - β) * c\n",
        "        for p, c\n",
        "        in zip(prev, curr)\n",
        "    ]\n",
        "    \n",
        "class AdamOptimizer:\n",
        "    def __init__(self, α=0.001, β1=0.9, β2=0.999, ϵ=1e-8):\n",
        "        self.α = α\n",
        "        self.β1 = β1\n",
        "        self.β2 = β2\n",
        "        self.ϵ = ϵ\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def send(self, gradients):\n",
        "        if self.m is None:\n",
        "            self.m = [0] * len(gradients)\n",
        "        if self.v is None:\n",
        "            self.v = [0] * len(gradients)\n",
        "\n",
        "        self.t += 1\n",
        "        αt = self.α * np.sqrt(1 - self.β2**self.t) / (1 - self.β1**self.t)\n",
        "        self.m = average(self.m, gradients, self.β1)        \n",
        "        self.v = average(self.v, (g*g for g in gradients), self.β2)\n",
        "\n",
        "        updates = [-αt * mi / (np.sqrt(vi) + self.ϵ) for mi, vi in zip(self.m, self.v)]\n",
        "        for upd in updates:\n",
        "            assert np.isfinite(upd).all()\n",
        "        return updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "wLem3_4ycUTm",
        "outputId": "85b3fbb5-473e-45b2-ceb2-97a4cf4dc57f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.0, -0.0, -0.0]\n",
            "[-0.0007441365882503311, -0.0007441365882503311, -0.0007441365882503311]\n",
            "[-0.0008584623637594434, -0.0008584623637594434, -0.0008584623637594434]\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamOptimizer()\n",
        "print(optimizer.send([0, 0, 0]))\n",
        "print(optimizer.send([1, 1, 1]))\n",
        "print(optimizer.send([1, 1, 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC2_ti0pcUTn"
      },
      "source": [
        "# Training the FFN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PENkXKMWcUTn"
      },
      "source": [
        "Our `Trainer` is a bit different now, as it calls the  optimizer at each step to determine the updates. Otherwise it's fairly similar to the one we used in the [softmax model session](softmax-model.ipynb#Training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "n1LqPbamcUTn"
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "\n",
        "def trainer(Ws, X, Y, optimizer, batch_size=batch_size, keep_prob=1):    \n",
        "    nsamples = X.shape[0]\n",
        "    batch = 0\n",
        "    while True:\n",
        "        # get next batch\n",
        "        start = (batch * batch_size) % nsamples\n",
        "        stop = start + batch_size\n",
        "        batch_idx = range(start, stop)\n",
        "        X_, Y_ = X[batch_idx, :], Y[batch_idx, :]\n",
        "        \n",
        "        # compute gradients\n",
        "        gradients = back_propagation(Ws, X_, Y_, keep_prob=keep_prob) \n",
        "        \n",
        "        # compute updates\n",
        "        ΔWs = optimizer.send(gradients) \n",
        "        \n",
        "        # apply updates\n",
        "        for W, ΔW in zip(Ws, ΔWs): \n",
        "            W += ΔW\n",
        "\n",
        "        # update iteration and yield\n",
        "        batch += 1\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv4raOvncUTo"
      },
      "source": [
        "Now we initialize two layers and calculate the inital accuracy.\n",
        "\n",
        "The input layer has 728 features, \"neurons\" or pixels; the readout later has 10 (categories).\n",
        "Let's set the hidden layer to have 100 \"neurons\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "w5Ns5eJ2cUTo",
        "outputId": "7d42a346-84ca-4684-9097-61b88a37ff6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.1184\n"
          ]
        }
      ],
      "source": [
        "nhidden = 100\n",
        "W1 = random_matrix(nfeatures, nhidden)\n",
        "W2 = random_matrix(nhidden, ncats)\n",
        "Ws = [W1, W2]\n",
        "\n",
        "test_acc = [accuracy(predict(Ws, X_test), Y_test)]\n",
        "print(\"Accuracy: {:.4f}\".format(test_acc[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jk__O4QcUTo"
      },
      "source": [
        "The accuracy we get is something around 0.1, depending on the specific random values of `W1` and `W2`.\n",
        "\n",
        "Let's train the model for 10 epochs (each time we go over all the images is called an *epoch*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "L9qOUYJ-cUTp",
        "outputId": "a4c2a541-3332-49ad-9d2d-703cb56b1fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (12000): 0.0980\n"
          ]
        }
      ],
      "source": [
        "train = trainer(Ws, X_train, Y_train, optimizer=MomentumOptimizer(), keep_prob=0.5)\n",
        "\n",
        "for batch in train:\n",
        "    if batch * batch_size % nsamples == 0: test_acc.append(accuracy(predict(Ws, X_test), Y_test))\n",
        "    if batch == 10 * nsamples // batch_size: break\n",
        "\n",
        "print(\"Accuracy ({:d}): {:.4f}\".format(batch, test_acc[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgmTegTHcUTp"
      },
      "source": [
        "Let's try with the Adam optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "Tylh3jvVcUTp",
        "outputId": "069e7fba-844f-48af-eed8-e42cbb91a83d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.1099\n"
          ]
        }
      ],
      "source": [
        "nhidden = 100\n",
        "W1 = random_matrix(nfeatures, nhidden)\n",
        "W2 = random_matrix(nhidden, ncats)\n",
        "Ws = [W1, W2]\n",
        "\n",
        "test_acc = [accuracy(predict(Ws, X_test), Y_test)]\n",
        "print(\"Accuracy: {:.4f}\".format(test_acc[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "uQO8xPTFcUTq",
        "outputId": "98ede96c-ddfa-467e-8ea0-734c93ce0015",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (12000): 0.9206\n"
          ]
        }
      ],
      "source": [
        "train = trainer(Ws, X_train, Y_train, optimizer=AdamOptimizer(), keep_prob=0.5)\n",
        "\n",
        "for batch in train:\n",
        "    if batch * batch_size % nsamples == 0: test_acc.append(accuracy(predict(Ws, X_test), Y_test))\n",
        "    if batch == 10 * nsamples // 50: break\n",
        "\n",
        "print(\"Accuracy ({:d}): {:.4f}\".format(batch, test_acc[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCq7KZ1AcUTq"
      },
      "source": [
        "So with Adam we got a rapid and significant increase in accuracy over the single-layer softmax model, which was ~89% after 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "YNJJt9PicUTq",
        "outputId": "d6fc3e23-6077-4ac9-b8b3-a8e8d2c6dc72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display_prediction(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wnzZFqlcUTr"
      },
      "source": [
        "Saving the model is just a matter of saving `Ws`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "A2qvkuwfcUTr",
        "outputId": "1007a770-9de4-428a-a8c4-65eb3dff00bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Are you sure?y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:719: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  val = np.asanyarray(val)\n"
          ]
        }
      ],
      "source": [
        "if input(\"Are you sure?\") == 'y':\n",
        "    np.savez_compressed('./FFN_MNIST_2layer.npz', Ws=Ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "KYVKN7VycUTr"
      },
      "outputs": [],
      "source": [
        "with np.load('./FFN_MNIST_2layer.npz', allow_pickle=True) as d:\n",
        "    Ws = d['Ws']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "xE6bOvlecUTs",
        "outputId": "de3721c5-30e6-426e-d7fb-babd71884ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADtCAYAAABTTfKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHX0lEQVR4nO3dP2iVdx/G4Sf+jRapCoEUFIdOLQ5CFYUqWA3ROjkoggGhRXHpUhEsTu0oBRUFi26KDlYQcbG0i6JuDgqBzupgaoqLCGlpet6hWLS853s4zTHJba9r9OZJf1U+PK0/TtLXarVaDRBjzkwfAOiOaCGMaCGMaCHMvG4fmJiYaEZHR5uBgYFm7ty5b+JM8J83OTnZjI+PN6tXr276+/tf27qOdnR0tBkZGenZ4YD2Ll261Kxdu/a1X+s62oGBgb+/2ODgYG9OBrxmbGysGRkZ+bu3V3Ud7cv/JB4cHGxWrFgx9dMBbf2//wX1F1EQRrQQRrQQRrQQRrQQRrQQRrQQRrQQRrQQRrQQRrQQRrQQRrQQputP+TAzvvnmm3I/f/58uX///ffl/s/PbDJ7edNCGNFCGNFCGNFCGNFCGNFCGFc+s8jNmzfbbmfPni2ffeedd8r93r175e7KJ4c3LYQRLYQRLYQRLYQRLYQRLYQRLYRxTzuNnj9/Xu67d+9uu+3bt6989tixY+Xe19dX7uTwpoUwooUwooUwooUwooUwooUwooUw7mmn0XfffVfu/f39bbfDhw+Xz86b54/yv8KbFsKIFsKIFsKIFsKIFsKIFsKIFsK43JtGnT7zevDgwbbbe++91+vjEMqbFsKIFsKIFsKIFsKIFsKIFsK48umhTt8i9ffffy/3Dz74oJfH4S3lTQthRAthRAthRAthRAthRAthRAth3NP20A8//DCl57dv396jk/A286aFMKKFMKKFMKKFMKKFMKKFMKKFMO5pe6jTj7JcsGBBuQ8MDPTyOLylvGkhjGghjGghjGghjGghjGghjGghjHvaLrRarXJ/9uxZuQ8NDfXyOLPGzZs3y/3y5ctT+vpLly5tu23atKl89tNPPy33vr6+f3WmmeRNC2FEC2FEC2FEC2FEC2FEC2FEC2Hc03bhyZMn5f7gwYNyP3LkSC+P01OdfnbuV1991XY7ceJE+eyqVavKfcmSJeW+cuXKttuZM2fKZ69cuVLuw8PD5T4bedNCGNFCGNFCGNFCGNFCGNFCGFc+02gmv0Xqn3/+We4HDhwo9wsXLrTdOn3r2M8++6zcFy5cWO6Va9eulfvBgwfL/f79++X+7rvvdn2mN82bFsKIFsKIFsKIFsKIFsKIFsKIFsK4p+3Cw4cPp/T8unXrenSS7n3xxRfl/uOPP5b7Tz/91HbbunVr+eyb/Dal27ZtK/eJiYlyf/HiRbm7pwWmTLQQRrQQRrQQRrQQRrQQRrQQxj1tF54+fTrTR2hrbGys3K9fv17uly5dKvctW7Z0fabpsGjRonJ///33y/327dvlvmfPnq7P9KZ500IY0UIY0UIY0UIY0UIY0UIY0UIY97RdWLBgwZSef/z4cblP5bObFy9eLPdffvml3D/++ON//c9O9vz585k+Qte8aSGMaCGMaCGMaCGMaCGMaCGMaCGMe9oubNy4sdwHBwfL/ezZs+V++vTprs/00oYNG8r9jz/+KPdbt26V+/DwcNdnmg6d/r063cMuXbq0l8eZFt60EEa0EEa0EEa0EEa0EEa0EMaVTxeWLFlS7itWrCj3K1eulPuJEyfabvPm1X9Uy5cvL/dOP26y09XJbHXq1Kly7/StZTv9mM7ZyJsWwogWwogWwogWwogWwogWwogWwrin7aHDhw+X+969e8v9yy+/bLt1+tjehx9+WO779++f0v7555+33fr7+8tnO9m0aVO5P3r0qO129OjR8tkbN26U+7Jly8p9NvKmhTCihTCihTCihTCihTCihTCihTDuaXtoz5495X716tVyP3fuXNtt8eLF5bOHDh0q906fO92xY0e5//rrr223VqtVPvvbb7+Ve6fflwcPHrTd7t69Wz770UcflXsib1oII1oII1oII1oII1oII1oII1oI4552Gl28eLHcq8+GHj9+vHz28uXL5b5z585yX7lyZblXrl27Vu537twp96GhoXI/efJk223NmjXls28jb1oII1oII1oII1oII1oII1oI48pnGs2fP7/cv/3227bbrl27ymc7Xfncvn273H/++edy37x5c9tty5Yt5bNff/11uX/yySflPmeOd8ur/G5AGNFCGNFCGNFCGNFCGNFCGNFCGPe0IdavXz+lnbeHNy2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EES2EmdftA5OTk03TNM3Y2FjPDwP85WVfL3t7VdfRjo+PN03TNCMjI1M8FtDJ+Ph4s2rVqtd+ra/VarW6+SITExPN6OhoMzAw0MydO7enBwT+Mjk52YyPjzerV69u+vv7X9u6jhaYWf4iCsKIFsKIFsKIFsKIFsKIFsKIFsKIFsKIFsKIFsL8D+tAKCPMf1TtAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display_prediction(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WVY_BdFcUTs"
      },
      "source": [
        "# References\n",
        "\n",
        "- [Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville](http://www.deeplearningbook.org), ch. 6, 7.12, and 8 - offers a comprehensive introduction to the subject.\n",
        "- [Deep Learning: Technical introduction by Thomas Epelbaum](https://github.com/tomepel/Technical_Book_DL), ch. 4 - offers a very technical perspective with full derivation of all the formulas.\n",
        "\n",
        "Both books are available online for free."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3nAL1VEcUTs"
      },
      "source": [
        "# Colophon\n",
        "This notebook was written by [Yoav Ram](http://python.yoavram.com) and is part of the [_Scientific Computing with Python_](https://scicompy.yoavram.com/) course at IDC Herzliya.\n",
        "\n",
        "The notebook was written using [Python](http://python.org/) 3.6.5.\n",
        "Dependencies listed in [environment.yml](../environment.yml).\n",
        "\n",
        "This work is licensed under a CC BY-NC-SA 4.0 International License.\n",
        "\n",
        "![Python logo](https://www.python.org/static/community_logos/python-logo.png)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:SciComPy]",
      "language": "python",
      "name": "conda-env-SciComPy-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "FFN.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}